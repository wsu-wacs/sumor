---
title: "Simulating Road traffic is SUMO and OSM"
author: "Matthew Piekenbrock"
output: html_notebook
---

This document details how to run a complex simulation in SUMO with the sumor package, 
with both vehicular and pedestrian indoor and outdoor routing, 
using an imported map from OpenStreetMap. The simulaton creates traffic over a small portion of the Ohio State Universities (OSUs)
campus. 

```{r, message=FALSE, results='hide', include=FALSE, cache=TRUE}
  ## Load library, set SUMO home path 
  suppressMessages({ library("sumor", quietly = T) })
  Sys.setenv(SUMO_HOME="/usr/local/Cellar/sumo/0.29.0")
  
  ## Create temporary directory to store all the intermediate data files (optional)
  if (!dir.exists("./tmp")){ dir.create("./tmp") }
  Sys.setenv(SUMO_TMP=paste0(getwd(),"/tmp/"))
  
  ## Import or make a sumo(R) object 
  osu_net <- if ("osu_net" %in% ls()) sumor::sumo$new()$import(osu_net) else sumor::sumo$new() 
  
  ## Get OSM Map using bounding box (Selected from http://boundingbox.klokantech.com )
  ## WSU: matrix(c(-84.077983,39.775924,-84.058666,39.786939), ncol=2, byrow=T)
  ## OSU: matrix(c(-83.0179685354,39.9974324509,-83.0098897219,40.0018952624), ncol=2, byrow = T)
  ## Google Headquarters: matrix(c(-122.0921051502,37.4165612899,-122.0812475681,37.4247242124, ncol=2, byrow = T))
  osu_bbox <- sp::bbox(matrix(c(-83.026471,39.992056,-83.005422,40.00681), ncol=2, byrow = T))
  osu_net$getOSM(osu_bbox, file = "osu.osm", overwrite = T)
  
  ## Convert the source representation to a SUMO road network 
  osu_net$netconvert(urban = T, pedestrian = T, polygons = T, flags = "--sidewalks.guess --crossings.guess TRUE")
  
  ## -- SUMOR-specific functionality -- 
  ## Convert OSM polygons to pedestrian-only junctions (connected by closest pedestrian-enabled edges)
  osu_net$addPolys(buildings_only = T)
```

## Visualizing the geospace
```{r}
## Plotting the junctions via leaflet 
suppressMessages({ library("leaflet", quietly = T) })
# polys <- lapply(b_xy, function(bxy) osu_net$projectCRS(bxy)@coords)

## Size of the geospace (kilometers)
bbox_size <- function(bbox){
  toMeters <- function(lat1, lon1, lat2, lon2){
    R = 6378.137 ## Radius of earth in KM
    dLat = lat2 * pi / 180 - lat1 * pi / 180
    dLon = lon2 * pi / 180 - lon1 * pi / 180
    a = sin(dLat/2) * sin(dLat/2) +
      cos(lat1 * pi / 180) * cos(lat2 * pi / 180) *
      sin(dLon/2) * sin(dLon/2)
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    d = R * c
    return(d * 1000) #  meters
  }
  x <- toMeters(lon1 = bbox[1, 1], lon2 = bbox[1, 2], lat1 = bbox[2, 1], lat2 = bbox[2, 1])
  y <- toMeters(lon1 = bbox[1, 1], lon2 = bbox[1, 1], lat1 = bbox[2, 1], lat2 = bbox[2, 2])
  cat("Size of geospace: ", ((x * y) / 1000), "km^2 \n")
  return(c(x=x, y=y))
}

plotLeaflet <- function(bbox, map_source = c("arcgis", "osm", "mapbox"), color_palette = palette(rainbow(10))){
  ## Set vector tile source
  if (missing(map_source) || map_source == "arcgis"){
    map_template <- "http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
  } else if (map_source == "osm"){
    map_template <- "http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png"
  } else if (map_source == "mapbox"){
    map_template <- "https://api.mapbox.com/styles/v1/peekxc2772/cj2uqbuwl00002rpa0ao2f8r4/tiles/256/{z}/{x}/{y}@2x?access_token=pk.eyJ1IjoicGVla3hjMjc3MiIsImEiOiJjaWtlb3loaTkwMDV1djJtMms2NmJzem52In0.nph9xPOodDeWnF58qb8MDw"
  } else {
    map_template <- map_source
  }
  palette(color_palette)
  m <- leaflet() %>% 
    setView(lng = apply(bbox, 1, mean)[[1]], lat = apply(bbox, 1, mean)[[2]], zoom = 16) %>% 
    addTiles(urlTemplate=map_template)
  m
}

```

## Generating demand
Every SUMO simulation needs a duration to run the simulation for. SUMO defines the simulation length with respect to seconds. 
```{r}
## Helper functions converting minutes and hours to seconds
{ minutes <- function(m) m * 60; hours <- function(h) h * 60 * 60 }
```

Often, it's desirable to make a simulation over a long period of time, to which various statistics are to be extracted from the simulatin at various steps. The difficulty is that there are a large variety of different output formats to export from a given simulation, each of which may contain hundreds of thousands of records. It's often the case that only a very small subset this data is needed for the application at hand. 

Thus, for efficiency, it's useful to process the simulation in batch mode, extracting the desired information from the simulation over small intervals. Below will configure a road traffic simulation for one hour, processing the output every 15 minutes: 

```{r}
suppressMessages({ library("data.table", quietly = T) })
## How much time to simulate for? (batch mode: 15 minutes)
sim_length <- hours(2)
increment <- hours(1) / 4
sim_intervals <- data.table::data.table(start=seq(0, sim_length - increment, by = increment), 
                                        end=seq(increment, sim_length, by = increment) - 1)
```

Assuming one has a simulation prepared, the next step is to define what information is important to extract from the simulation. The traffic scenario 

```{r}
extractExemplars <- function(pt_list, sim_intervals, extract_type){
  for (i in 1:nrow(sim_intervals)) {
    ## Running the simulation generated with randomTrips (0.01 == 10ms intervals)
    osu_net$genConfig(begin = sim_intervals[i]$start, end = sim_intervals[i]$end)
    osu_net$simulate(flags = "--step-length 0.25 --fcd-output tmp_fcd.xml --fcd-output.signals")
  
    ## get 'floating car data' (records pedestrian movements as well)
    fcd_output <- osu_net$getFCD("tmp_fcd.xml")
  
    # When did the vehicle or person stop?
    if (extract_type == "vehicle" || "lane" %in% colnames(fcd_output)) {
      pt_list[[i]]$vehicle  <- unique(fcd_output[signals >= 8, ## Extract points where the vehicle was braking   
                                          .SD[which(.SD[["speed"]] == min(.SD[["speed"]])), ## ...and the vehicle was at it's slowest
                                          .(x, y)], by=.(id, lane)]) ## ...per vehicle, and per lane
    } else if (extract_type == "pedestrian" || "edge" %in% colnames(fcd_output)){
      pt_list[[i]]$person <- unique(fcd_output[grep(edge, pattern = "e[[:digit:]]+")][speed == 0, .(x, y), by=.(id, edge)])
    }
  } 
  return(pt_list)
}
```


```{r}
## Create list to hold both vehicle and person results
pt_list <- lapply(1:nrow(sim_intervals), function(i) list(person=list(), vehicle=list()))

## Generate n vehicle arrivals per second w/ probability 1/p
osu_net$randomTrips(start = 0, end = hours(1), n = 1, p = 10, flags = "--seed 1234") 

## Extract the 'stay points' where vehicles stopped 
pt_list <- extractExemplars(pt_list, sim_intervals, extract_type = "vehicle")

## Generate and augment the pedestrian routes 
osu_net$randomTrips(start = 0, end = sim_length, n = 1, p = 20, person = TRUE, flags = "--seed 1234")

## SUMO doesn't support pedestrians stopping; augment the routes s.t. pedestrian stop 
## randomly inside buildings along their route
osu_net$addStops()

## Extract the 'stay points' where pedestrians stopped 
pt_list <- extractExemplars(pt_list, sim_intervals, extract_type = "pedestrian")
```

SUMO can generate many different types of multimodal types of traffic demand. The car traffic, at a microscopic level, is very realistic; traffic jams caused by over congestion, realistic traffic light conditions, right-of-way dynamics, etc. 

## Viewing the simulation 
```{r, eval=FALSE}
  osu_net$viewSimulation()
```


```{r}
## Combine into one data.table (every 4 increments == 1 hour)
t_i <- 1:length(pt_list)

## Vehicle stay points; disregard internal lane stops
sp_vehicle <- data.table::rbindlist(lapply(pt_list[t_i], function(bp_list) bp_list$vehicle))
sp_vehicle <- sp_vehicle[!(lane %in% grep(x = lane, pattern = "^:.*", value = T))][,.(lane, x, y)]

## Pedestrian stay points
sp_person <- data.table::rbindlist(lapply(pt_list[t_i], function(bp_list) bp_list$person))

## Visualize samples using leaflet
# stay_pts <- osu_net$projectCRS(as.matrix(brake_pts[, .(as.numeric(x), as.numeric(y))]))@coords
#m %>% addCircles(lng = stay_pts_smpl[,1], lat = stay_pts_smpl[,2], radius = 1, color = "red")
#m %>% addPolygons(lng = osu_net$projectCRS(what)@coords[, 1], lat = osu_net$projectCRS(what)@coords[, 2])
```

# Creating truth labels

```{r}
## Create cluster IDs based on semantics of application at hand; 
## Cluster vehicles by lane or by intersection? 
by_intersection <- TRUE

## Get edge ids 
edges <- osu_net$getEdges()
edge_ids <- xml2::xml_attr(edges, "id")

if (by_intersection){
  ## By intersection
  eids <- sub(sp_vehicle$lane, pattern = "(.+)_[[:digit:]]+", replacement = "\\1")
  edge_indices <- sapply(eids, function(eid) which(edge_ids == eid))
  target_nodes <- sapply(edges, function(edge) xml2::xml_attr(edge, "to"))[unname(edge_indices)]
  cids <- match(target_nodes, unique(target_nodes))
} else {
  ## By lane 
  lane_ids <- unique(sp_vehicle$lane)
  cids <- match(sp_vehicle$lane, lane_ids)
}

## Vehicles are assigned cluster ids using above
truth_vehicle <- cbind(sp_vehicle, cid = cids)

## Pedestrians based on the building they're in (offset so clusters are disjoint from vehicles assignments)
base_edges <- sub(sp_person$edge, pattern = "(e[[:digit:]]+)_[[:digit:]]+", replacement = "\\1")
truth_pedestrian <- cbind(sp_person, cid = match(base_edges, edge_ids) + max(truth_vehicle$cid))

## Plot the results 
# results <- rbind(truth_vehicle[, .(x, y, cid)], truth_pedestrian[, .(x, y, cid)])
# plot(results[, .(x, y)], col=results$cid, cex=0.5, pch=20)
```

### Extracting junction data 
```{r}
## Function to turn junction xy into lat long coordinates 
junc_to_lnglat <- function(junctions, sumor_net){
  juncs <- data.table::rbindlist(lapply(junctions, function(junc) junc[c("x", "y")]))
  juncs <- juncs[, lapply(.SD, as.numeric)]
  sumor_net$projectCRS(as.matrix(juncs))@coords
}

## Getting the junction data (filter out internal junctions)
osu_junctions <- osu_net$getJunctions()
pj <- Filter(function(junc) junc$type != "internal", osu_junctions)

## Get plain junction (x,y) SUMO coordinates 
pj_xy <- data.table::rbindlist(lapply(pj, function(junc) junc[c("x", "y")]))[, lapply(.SD, as.numeric)]

## Parse through the junctions, getting all of the incoming and internal roads
edge_regex <- "[:-]?([[:digit:]]+)(?:[_#][[:digit:]]*)*"
inc_roads <- lapply(pj, function(junc) sub(unlist(strsplit(junc[["incLanes"]], " ")), pattern = edge_regex, replacement = "\\1"))
int_roads <- lapply(pj, function(junc) sub(unlist(strsplit(junc[["intLanes"]], " ")), pattern = edge_regex, replacement = "\\1"))
                    
## For each junction, assign a cluster ID to the roads it's connected it
road_ids <- unique(c(unlist(inc_roads), unlist(int_roads)))
road_map <- structure(vector(mode = "list", length = length(road_ids)), names=road_ids)
for (i in 1:length(pj)){
  for (road_id in c(inc_roads[[i]], int_roads[[i]])){ 
    road_map[[road_id]] <- unique(c(road_map[[road_id]], i))
  }
}
```

### Creating truth labels 
```{r}
## Parse brake point lanes into roads 
brake_pt_roads <- sub(brake_pts$lane, pattern = edge_regex, replacement = "\\1")

## Make sure every brake point has a road associated with it
all(brake_pt_roads %in% names(road_map)) # [1] TRUE

## Get the `true' cluster assignments by junction. In case a road is associated with multiple junctions, 
## assign the brake point to the nearest junction connected to the road its on (takes awhile)
true_cl <- sapply(1:nrow(brake_pts), function(i) {
  cl_id <- road_map[[brake_pt_roads[i]]]
  ifelse(length(cl_id) > 1, 
    cl_id[class::knn1(pj_xy[cl_id], brake_pts[i, .(x, y)], seq(nrow(pj_xy[cl_id])))],
    cl_id)
})
```

Visualizing the truth data
```{r}
suppressMessages({ library("leaflet", quietly = T) })
## Get plain, internal, and trafficked (plain) junction WSG84 coordinates
pj_pts <- junc_to_lnglat(pj, osu_net)

## Extract WSG84 coordinates from points; get random sample to reduce plotting time
stay_pts <- osu_net$projectCRS(as.matrix(brake_pts[, .(as.numeric(x), as.numeric(y))]))@coords

## Visualize the colors on the map (with color redundancy)
## Note: uncomment first circles command to see SUMO junctions
palette("default")
m %>% 
  addCircles(lng = pj_pts[unique(true_cl),1], lat=pj_pts[unique(true_cl),2], radius = 4, color = "orange") %>%
  addCircles(lng = stay_pts[,1], lat = stay_pts[,2], radius = 1, 
             color = palette()[(true_cl %% length(palette()))+1L])
```


## Saving the data 
```{r}
## Save off simulation data for loading in later 
sumo_truth <- list(cluster=results$cid, sp=as.matrix(results[, .(x, y)]))
save(sumo_truth, file = paste0(getwd(), "/sumo_truth.rdata"))
```

## All saved in file 
```{r}
source('~/WaCS/reporting/CIKM2017 - ClusterTree POIs/CIKM2017.R')
## Helper functions converting minutes and hours to seconds
{ minutes <- function(m) m * 60; hours <- function(h) h * 60 * 60 }
getOneMeterLatLong <- function(bbox) {
  xy <- bbox_size(bbox)
  one_meter <- sqrt((bbox[1, 2] - bbox[1, 1])^2 + (bbox[2, 2] - bbox[2, 1])^2) * 1/sqrt(xy[["x"]]^2 + xy[["y"]]^2)
  one_meter
}
## OSU Tests 
osu_bbox <- matrix(c(-83.0179685354,39.9974324509,-83.0098897219,40.0018952624), ncol=2, byrow = T)
osu_truth <- IntrinsicPOIs(osu_bbox, sim_length = hours(8), 
                           ped_arr_rate = list(n = 5, p = 10), 
                           veh_arr_rate = list(n = 5, p = 10), 
                           osm_file = "osu.osm")

## View the truth results geospace
osu_net <- osu_truth$vars[["sumo_net"]]
one_meter <- getOneMeterLatLong(sp::bbox(osu_bbox))
exemplars <- osu_net$projectCRS(osu_truth$results$sp)@coords
exemplars <- exemplars + matrix(replicate(2, runif(nrow(exemplars), min = 0, max = one_meter*5)), ncol=2)
palette(rainbow(10))
plotLeaflet(sp::bbox(osu_bbox)) %>% 
  addCircles(lng = exemplars[, 1], lat = exemplars[, 2], col = palette()[osu_truth$results$cluster %% 10 + 1L], 
             radius=2.5, stroke = FALSE)

## WSU Tests 
wsu_bbox <- matrix(c(-84.066106081,39.7784222828,-84.0600550175,39.7842267811), ncol=2, byrow=T)
#xy <- bbox_size(sp::bbox(wsu_bbox))
#one_meter <- sqrt((wsu_bbox[1, 2] - wsu_bbox[1, 1])^2 + (wsu_bbox[2, 2] - wsu_bbox[2, 1])^2) * 1/sqrt(xy[["x"]]^2 + xy[["y"]]^2)
wsu_truth <- IntrinsicPOIs(wsu_bbox, sim_length = hours(12), 
                           ped_arr_rate = list(n = 10, p = 2),
                           veh_arr_rate = list(n = 1, p = 30),
                           osm_file = "wsu.osm")
plotMap(wsu_truth)
plotLeaflet(wsu_bbox) %>% 
  exemplars <- osu_net$projectCRS(osu_truth$results$sp)@coords
  exemplars <- exemplars + matrix(replicate(2, runif(nrow(exemplars), min = 0, max = one_meter*5)), ncol=2)
  addCircles(lng = exemplars[, 1], lat = exemplars[, 2], col = palette()[osu_truth$results$cluster %% 10 + 1L], 
             radius=2.5, stroke = FALSE)
cat("Number of points: ", nrow(wsu_truth$results$sp), "(", length(unique(wsu_truth$results$cluster)), " clusters)\n")

## Google Tests 
google_bbox <- matrix(c(-122.0921051502,37.4165612899,-122.0812475681,37.4247242124), ncol=2, byrow = T)
bbox_size(sp::bbox(google_bbox))
google_truth <- IntrinsicPOIs(google_bbox, sim_length = hours(8), 
                              ped_arr_rate = list(n = 2, p = 10),
                              veh_arr_rate = list(n = 2, p = 10), 
                              osm_file = "google.osm")
cat("Number of points: ", nrow(google_truth$results$sp), "(", length(unique(google_truth$results$cluster)), " clusters)\n")

```

Compute validation measures 

```{r}

## Interchange which simulation to use
test_set <- google_truth #  google_truth$results
# bbox_size(test_set$vars[["sumo_net"]]$config$bbox)
sumo_truth <- with(test_set$results, {
  ## Add roughly x meters of random noise to each observation to simulate GPS inaccuracy
  test_bb <- test_set$vars[["sumo_net"]]$config$bbox
  test_xy <- bbox_size(test_bb)
  one_meter <- sqrt((test_bb[1, 2] - test_bb[1, 1])^2 + (test_bb[2, 2] - test_bb[2, 1])^2) * 1/sqrt(test_xy[["x"]]^2 + test_xy[["y"]]^2)
  sp <- sp + matrix(replicate(2, runif(nrow(sp), min = 0, max = one_meter*10)), ncol=2)
  list(sp=sp, cluster=cluster)
})

plotMap(test_set, test_set$results)

save(sumo_truth, file = "wsu_truth.rdata")
```

## Plotting the truth data 
```{r, echo = FALSE}
## Taken from URL: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/ 
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


## Validating 
Validation measures used 
```{r validate_function, cache=TRUE, include=FALSE}
suppressMessages({ library("mcclust"); library("dendextend") })
validate <- function(test_cl, truth_cl){
  return(list(
    VI = 1 - (mcclust::vi.dist(test_cl, truth_cl) / log(length(test_cl), base = 2)), # normalized complement of VI
    FM = as.vector(dendextend::FM_index_R(test_cl, truth_cl)), 
    ARI = mclust::adjustedRandIndex(test_cl, truth_cl)
  ))
} 
```



```{r, cache=TRUE}
library("clustertree")

## record top 10 
top <- 10

## Precompute distance matrix 
dist_sp <- dist(sumo_truth$sp, method = "euclidean")

## Enumerate possible settings of k up to # of true clusters (inclusive) 
k_values <- seq(2, length(unique(sumo_truth$cluster)), by = 1)

## Enumerate minPts through quantiles of the size of the clusters; make sure they're all > 1 
minPts_values <- quantile(table(sumo_truth$cluster), probs = seq(0.15, 0.95, 0.05))
minPts_values[floor(minPts_values) == 1] <- minPts_values[which(floor(minPts_values) == 1)] + 1L

## Using K-means 
km <- lapply(k_values, function(k) kmeans(sumo_truth$sp, centers = k)$cluster)
km <- Filter(km, f = function(km_cl) !any(table(km_cl) == 1))
km_ari <- t(sapply(km, function(km_cl) validate(km_cl, sumo_truth$cluster)))
km_ari <- apply(km_ari, 2, unlist)

## Using the cluster tree 
rsl_trees <- lapply(minPts_values, function(k) clustertree::clustertree(dist_sp, k=k, alpha = sqrt(2)))
rsl <- mapply(rsl_trees, minPts_values, FUN = function(tree, k) dbscan::extractFOSC(tree, minPts = k), SIMPLIFY = F)
rsl <- Filter(rsl, f = function(rsl_cl) !any(length(table(rsl_cl$cluster)) == 1))
rsl <- Filter(rsl, f = function(rsl_cl) sum(rsl_cl$cluster == 0) < length(rsl_cl$cluster) * 0.50) ## at least 50% of the point should be classified 
rsl_ari <- t(sapply(rsl, function(rsl_cl) validate(rsl_cl$cluster[rsl_cl$cluster != 0], sumo_truth$cluster[rsl_cl$cluster != 0])))
rsl_ari <- apply(rsl_ari, 2, unlist)

## Single Linkage 
sl <- hclust(dist_sp, method = "single")
sl_cl <- lapply(k_values, function(k) cutree(sl, k = k))
sl_cl <- Filter(sl_cl, f = function(sl_cl) !any(length(table(sl_cl)) == 1))
sl_ari <- t(sapply(sl_cl, function(sl_cl) validate(sl_cl, sumo_truth$cluster)))
sl_ari <- apply(sl_ari, 2, unlist)

## Average Linkage 
al <- hclust(dist_sp, method = "average")
al_cl <- lapply(k_values, function(k) cutree(sl, k = k))
al_cl <- Filter(al_cl, f = function(al_cl) !any(length(table(al_cl)) == 1))
al_ari <- t(sapply(al_cl, function(al_cl) validate(al_cl, sumo_truth$cluster)))
al_ari <- apply(al_ari, 2, unlist)

## Wards Linkage 
wl <- hclust(dist_sp, method = "ward.D2")
wl_cl <- lapply(k_values, function(k) cutree(wl, k = k))
wl_cl <- Filter(wl_cl, f = function(wl_cl) !any(length(table(wl_cl)) == 1))
wl_ari <- t(sapply(wl_cl, function(wl_cl) validate(wl_cl, sumo_truth$cluster)))
wl_ari <- apply(wl_ari, 2, unlist)

## Clara
cl <- lapply(k_values, function(k) cluster::clara(sumo_truth$sp, k = k))
cl <- Filter(cl, f = function(cl_cl) !any(length(table(cl_cl$clustering)) == 1))
cl_ari <- t(sapply(cl, function(cl_cl) validate(cl_cl$clustering, sumo_truth$cluster)))
cl_ari <- apply(cl_ari, 2, unlist)

## DBSCAN (Note that noise is discarded, and )
eps_values <- quantile(dist_sp, probs = seq(0.01, 0.20, by = 0.01))
params <- expand.grid(eps = eps_values, k = minPts_values)
db <- mapply(function(k, eps) dbscan::dbscan(sumo_truth$sp, eps=eps, minPts=k), params$k, params$eps, SIMPLIFY = F)
db <- Filter(db, f = function(db_cl) !any(length(table(db_cl$cluster)) == 1))
db <- Filter(db, f = function(db_cl) sum(db_cl$cluster == 0) < length(db_cl$cluster) * 0.50) 
db_ari <- t(sapply(db, function(db_cl) validate(db_cl$cluster[db_cl$cluster != 0], sumo_truth$cluster[db_cl$cluster != 0])))
db_ari <- apply(db_ari, 2, unlist)

## OPTICS-extractDBSCAN 
params <- expand.grid(eps = tail(eps_values, 1), k = minPts_values)
op <- mapply(function(k, eps) dbscan::optics(sumo_truth$sp, eps=eps, minPts=k), params$k, params$eps, SIMPLIFY = F)
params2 <- lapply(op, function(op) quantile(op$reachdist, probs =  seq(0.20, 0.80, 0.025)))
op_cl <- vector(mode = "list", length = length(op) * length(params2[[1]]))
i <- 1
for (op_i in 1:length(op)) {
  for (eps_cl in params2[[op_i]]){
    op_cl[[i]] <- dbscan::extractDBSCAN(op[[op_i]], eps_cl)$cluster
    i <- i + 1
  }
}
op_cl <- Filter(op_cl, f = function(op_cl) !any(length(table(op_cl)) == 1))
op_cl <- Filter(op_cl, f = function(op_cl) sum(op_cl == 0) < length(op_cl) * 0.50) 
op_ari <- t(sapply(op_cl, function(op_cl) validate(op_cl[op_cl != 0], sumo_truth$cluster[op_cl != 0])))
op_ari <- apply(op_ari, 2, unlist)

# sapply(rsl, function(rsl_cl) sum(rsl_cl$cluster == 0) / length(rsl_cl$cluster))
## BIC-optimized Gaussian Mixture Model (GMM) [probably unneeded] 
# sumoBIC <- mclust::mclustBIC(sumo_truth$sp)
# sumoGMM <- mclust::mclustModel(sumo_truth$sp, sumoBIC)

# Returns the top n ARI scores 
topARI <- function(x, n = 10){ x[order(x[, 3], decreasing = T), 3][1:n] }
## Results setup
results <- list(topARI(rsl_ari), topARI(sl_ari), topARI(al_ari),
                topARI(wl_ari), topARI(cl_ari), topARI(km_ari),
                topARI(db_ari), topARI(op_ari))
results <- structure(lapply(results, melt), names = c("RSL", "SL", "AL", "WL", "CLARA", "K-MEANS", "DBSCAN", "OPTICS"))
results <- structure(data.table::rbindlist(results, idcol = TRUE), names = c(".id", "score")) #"CVI"


## Save the results 
save(results, file = "osu_results.rdata")

## Results plotting
suppressMessages({ library("ggplot2", quietly = T) })
ari_plot <- vi_plot <- fm_plot <- ggplot2::ggplot() 

## Adjusted Rand Plot
ari_plot <- ari_plot + ggplot2::ggtitle("OSU Validation: ARI") +
  ggplot2::geom_boxplot(data = results[CVI == "ARI"], aes(x = .id, y = score)) + 
  ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                 axis.title.x = element_blank()) +
  ggplot2::labs(y = "Adjusted Rand Index") + ggplot2::ylim(c(0, 1))

## Variation of Information (normalized complement)
vi_plot <- vi_plot + ggplot2::ggtitle("OSU Validation: VI") +
  ggplot2::geom_boxplot(data = results[CVI == "VI"], aes(x = .id, y = score)) + 
  ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                 axis.title.x = element_blank()) +
  ggplot2::labs(y = "Variation of Information") + ggplot2::ylim(c(0.5, 1))

## Fowlkes–Mallows index
fm_plot <- fm_plot + ggplot2::ggtitle("OSU Validation: FM index") +
  ggplot2::geom_boxplot(data = results[CVI == "FM"], aes(x = .id, y = score)) + 
  ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                 axis.title.x = element_blank()) +
  ggplot2::labs(y = "Fowlkes-Mallows index") + ggplot2::ylim(c(0, 1))

multiplot(ari_plot, vi_plot, fm_plot, cols = 3)
  # { prev.par <- par(no.readonly = TRUE); par(mfrow = c(1, 3)); palette(sample(viridis::viridis(7))) }
  # par(prev.par)


# ggplot2::ggplot(data = results[CVI == "ARI", .(score = max(score)), by=.id], aes(x = .id, y = score)) + 
#   ggplot2::ggtitle("SUMO Validation: Max ARI") + 
#   ggplot2::ylim(c(0, 1)) + 
#   ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
#                  axis.title.x = element_blank()) + 
#   ggplot2::geom_bar(stat = "identity")


```

```{r}
plotValidation(sumo_truth, sim_name){
  dist_sp <- dist(sumo_truth$sp, method = "euclidean")
  k_values <- seq(2, length(unique(sumo_truth$cluster)), by = 1)

  ## Using K-means 
  km <- lapply(k_values, function(k) kmeans(sumo_truth$sp, centers = k)$cluster)
  km <- Filter(km, f = function(km_cl) !any(table(km_cl) == 1))
  km_ari <- t(sapply(km, function(km_cl) validate(km_cl, sumo_truth$cluster)))
  km_ari <- apply(km_ari, 2, unlist)
  
  ## Using the cluster tree 
  rsl_trees <- lapply(k_values, function(k) clustertree::clustertree(dist_sp, k=k, alpha = sqrt(2)))
  rsl <- mapply(function(tree, k) dbscan::extractFOSC(tree, minPts = k), rsl_trees, k_values, SIMPLIFY = F)
  rsl <- Filter(rsl, f = function(rsl_cl) !any(length(table(rsl_cl$cluster)) == 1))
  rsl_ari <- t(sapply(rsl, function(rsl_cl) validate(rsl_cl$cluster[rsl_cl$cluster != 0], sumo_truth$cluster[rsl_cl$cluster != 0])))
  rsl_ari <- apply(rsl_ari, 2, unlist)
  
  ## Single Linkage 
  sl <- hclust(dist_sp, method = "single")
  sl_cl <- lapply(k_values, function(k) cutree(sl, k = k))
  sl_cl <- Filter(sl_cl, f = function(sl_cl) !any(length(table(sl_cl)) == 1))
  sl_ari <- t(sapply(sl_cl, function(sl_cl) validate(sl_cl, sumo_truth$cluster)))
  sl_ari <- apply(sl_ari, 2, unlist)
  
  ## Average Linkage 
  al <- hclust(dist_sp, method = "average")
  al_cl <- lapply(k_values, function(k) cutree(sl, k = k))
  al_cl <- Filter(al_cl, f = function(al_cl) !any(length(table(al_cl)) == 1))
  al_ari <- t(sapply(al_cl, function(al_cl) validate(al_cl, sumo_truth$cluster)))
  al_ari <- apply(al_ari, 2, unlist)
  
  ## Wards Linkage 
  wl <- hclust(dist_sp, method = "ward.D2")
  wl_cl <- lapply(k_values, function(k) cutree(wl, k = k))
  wl_cl <- Filter(wl_cl, f = function(wl_cl) !any(length(table(wl_cl)) == 1))
  wl_ari <- t(sapply(wl_cl, function(wl_cl) validate(wl_cl, sumo_truth$cluster)))
  wl_ari <- apply(wl_ari, 2, unlist)
  
  ## Clara
  cl <- lapply(k_values, function(k) cluster::clara(sumo_truth$sp, k = k))
  cl <- Filter(cl, f = function(cl_cl) !any(length(table(cl_cl$clustering)) == 1))
  cl_ari <- t(sapply(cl_cl, function(cl_cl) validate(cl_cl$clustering, sumo_truth$cluster)))
  cl_ari <- apply(cl_ari, 2, unlist)
  
  ## DBSCAN (Note that noise is discarded, and )
  eps_values <- quantile(dist_sp, probs = seq(0.01, 0.20, by = 0.01))
  params <- expand.grid(eps = eps_values, k = seq(2, length(unique(sumo_truth$cluster)), by = 5))
  db <- mapply(function(k, eps) dbscan::dbscan(sumo_truth$sp, eps=eps, minPts=k), params$k, params$eps, SIMPLIFY = F)
  db <- Filter(db, f = function(db_cl) !any(length(table(db_cl$cluster)) == 1))
  db_ari <- t(sapply(db, function(db_cl) validate(db_cl$cluster[db_cl$cluster != 0], sumo_truth$cluster[db_cl$cluster != 0])))
  db_ari <- apply(db_ari, 2, unlist)
  
  ## OPTICS-extractDBSCAN 
  params <- expand.grid(eps = tail(eps_values, 1), k = seq(2, length(unique(sumo_truth$cluster)), by = 5))
  op <- mapply(function(k, eps) dbscan::optics(sumo_truth$sp, eps=eps, minPts=k), params$k, params$eps, SIMPLIFY = F)
  params2 <- lapply(op, function(op) quantile(op$reachdist, probs = seq(0.01, 0.50, by = 0.01)))
  op_cl <- list()
  for (op_i in 1:length(op)) {
    i <- 0
    for (eps_cl in params2[[op_i]]){
      op_cl[[op_i + i]] <- dbscan::extractDBSCAN(op[[op_i]], eps_cl)$cluster
      i <- i + 1
    }
  }
  op_cl <- Filter(op_cl, f = function(op_cl) !any(length(table(op_cl)) == 1))
  op_ari <- t(sapply(op_cl, function(op_cl) validate(op_cl[op_cl != 0], sumo_truth$cluster[op_cl != 0])))
  op_ari <- apply(op_ari, 2, unlist)

  ## BIC-optimized Gaussian Mixture Model (GMM) [probably unneeded] 
  # sumoBIC <- mclust::mclustBIC(sumo_truth$sp)
  # sumoGMM <- mclust::mclustModel(sumo_truth$sp, sumoBIC)

  ## Results setup
  models <- list(RSL=rsl)
  results <- list(rsl_ari, sl_ari, al_ari, wl_ari, cl_ari, km_ari, db_ari, op_ari)
  results <- structure(lapply(results, melt), names = c("RSL", "SL", "AL", "WL", "CLARA", "K-MEANS", "DBSCAN", "OPTICS"))
  results <- structure(data.table::rbindlist(results, idcol = TRUE), names = c(".id", "index", "CVI", "score"))

  ## Results plotting
  suppressMessages({ library("ggplot2", quietly = T) })
  ari_plot <- vi_plot <- fm_plot <- ggplot2::ggplot() 

  ## Adjusted Rand Plot
  ari_plot <- ari_plot + ggplot2::ggtitle(paste0(sim_name, "Validation: ARI")) +
    ggplot2::geom_boxplot(data = results[CVI == "ARI"], aes(x = .id, y = score)) + 
    ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                   axis.title.x = element_blank()) +
    ggplot2::labs(y = "Adjusted Rand Index") + ggplot2::ylim(c(0, 1))
  
  ## Variation of Information (normalized complement)
  vi_plot <- vi_plot + ggplot2::ggtitle(paste0(sim_name, "Validation: VI")) +
    ggplot2::geom_boxplot(data = results[CVI == "VI"], aes(x = .id, y = score)) + 
    ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                   axis.title.x = element_blank()) +
    ggplot2::labs(y = "Variation of Information") + ggplot2::ylim(c(0.5, 1))
  
  ## Fowlkes–Mallows index
  fm_plot <- fm_plot + ggplot2::ggtitle(paste0(sim_name, "Validation: FM index")) +
    ggplot2::geom_boxplot(data = results[CVI == "FM"], aes(x = .id, y = score)) + 
    ggplot2::theme(axis.text.x = element_text(angle = 60, hjust = 1), 
                   axis.title.x = element_blank()) +
    ggplot2::labs(y = "Fowlkes-Mallows index") + ggplot2::ylim(c(0, 1))
  
  multiplot(ari_plot, vi_plot, fm_plot, cols = 3)
  return(results)
}
```


